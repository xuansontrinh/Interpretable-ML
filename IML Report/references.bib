@book{molnar2019,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year       = {2019},
  subtitle   = {A Guide for Making Black Box Models Explainable}
}

@misc{south-german-credit-ds ,
author = "Ulrike Gr√∂mping",
year = "2019",
title = "South German Credit Data: Correcting a Widely Used Data Set",
url = "https://archive.ics.uci.edu/ml/datasets/South+German+Credit+%28UPDATE%29",
institution = "Beuth University of Applied Sciences Berlin" }

@misc{miller2018explanation,
      title={Explanation in Artificial Intelligence: Insights from the Social Sciences}, 
      author={Tim Miller},
      year={2018},
      eprint={1706.07269},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{NIPS2016_5680522b,
 author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Examples are not enough, learn to criticize! Criticism for Interpretability},
 url = {https://proceedings.neurips.cc/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf},
 volume = {29},
 year = {2016}
}
@report{Bergstra2012,
   abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
   author = {James Bergstra and James Bergstra@umontreal Ca and Yoshua Bengio@umontreal Ca},
   journal = {Journal of Machine Learning Research},
   keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
   pages = {281-305},
   title = {Random Search for Hyper-Parameter Optimization Yoshua Bengio},
   volume = {13},
   url = {http://scikit-learn.sourceforge.net.},
   year = {2012},
}